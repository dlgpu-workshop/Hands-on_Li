{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "760319e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 1456\n",
      "Number of validation samples: 202\n",
      "\n",
      "Number of training images: 1456\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'image' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 171\u001b[0m\n\u001b[0;32m    169\u001b[0m NUM_SAMPLES_TO_VISUALIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_SAMPLES_TO_VISUALIZE):\n\u001b[1;32m--> 171\u001b[0m     image, target \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    172\u001b[0m     visualize_sample(image, target)\n",
      "Cell \u001b[1;32mIn[6], line 39\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     36\u001b[0m min_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(image1)\n\u001b[0;32m     37\u001b[0m max_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(image1)\n\u001b[1;32m---> 39\u001b[0m scaled_image \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[0;32m     40\u001b[0m image \u001b[38;5;241m=\u001b[39m scaled_image\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     41\u001b[0m img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(image, dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'image' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob as glob\n",
    "from xml.etree import ElementTree as et\n",
    "from config import CLASSES, RESIZE_TO, TRAIN_DIR, VALID_DIR, BATCH_SIZE, IMAGE_TYPE\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import collate_fn, get_train_transform, get_valid_transform\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import tifffile as tif\n",
    "\n",
    "# the dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dir_path, width, height, classes, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.dir_path = dir_path\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.classes = classes\n",
    "\n",
    "        # get all the image paths in sorted order\n",
    "        self.image_paths = glob.glob(f\"{self.dir_path}/*.\"+IMAGE_TYPE)\n",
    "        self.all_images = [image_path.split('/')[-1] for image_path in self.image_paths]\n",
    "        self.all_images = sorted(self.all_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # capture the image name and the full image path\n",
    "        image_name = self.all_images[idx]\n",
    "        image_path = os.path.join(self.dir_path, os.path.split(image_name)[1])\n",
    "        # read the image\n",
    "        image = tif.imread(image_path)\n",
    "        #image = cv2.imread(image_path)\n",
    "#         min_val = np.min(image1)\n",
    "#         max_val = np.max(image1)\n",
    "\n",
    "        scaled_image = image/ 255.0\n",
    "        image = scaled_image.astype(np.float32)\n",
    "        img = np.asarray(image, dtype = np.float32)\n",
    "            # convert BGR to RGB color format\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "#         image_resized = cv2.resize(image, (self.width, self.height))\n",
    "#         image_resized /= 255.0\n",
    "        if img.ndim == 2:\n",
    "        # reshape (H, W) -> (1, H, W)\n",
    "            image_resized = img[np.newaxis]\n",
    "        else:\n",
    "        # transpose (H, W, C) -> (C, H, W)\n",
    "            image_resized =  img.transpose((2, 0, 1))\n",
    "\n",
    "\n",
    "        # capture the corresponding XML file for getting the annotations\n",
    "        annot_filename = image_name[:-4] + '.xml'\n",
    "        annot_file_path = os.path.join(self.dir_path, os.path.split(annot_filename)[1])\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        tree = et.parse(annot_file_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # get the height and width of the image\n",
    "        image_width = image.shape[1]\n",
    "        image_height = image.shape[0]\n",
    "\n",
    "        # box coordinates for xml files are extracted and corrected for image size given\n",
    "        for member in root.findall('object'):\n",
    "            # map the current object name to `classes` list to get...\n",
    "            # ... the label index and append to `labels` list\n",
    "            labels.append(self.classes.index(member.find('name').text))\n",
    "\n",
    "            # xmin = left corner x-coordinates\n",
    "            xmin = int(member.find('bndbox').find('xmin').text)\n",
    "            # xmax = right corner x-coordinates\n",
    "            xmax = int(member.find('bndbox').find('xmax').text)\n",
    "            # ymin = left corner y-coordinates\n",
    "            ymin = int(member.find('bndbox').find('ymin').text)\n",
    "            # ymax = right corner y-coordinates\n",
    "            ymax = int(member.find('bndbox').find('ymax').text)\n",
    "\n",
    "            # resize the bounding boxes according to the...\n",
    "            # ... desired `width`, `height`\n",
    "            xmin_final = (xmin / image_width) * self.width\n",
    "            xmax_final = (xmax / image_width) * self.width\n",
    "            ymin_final = (ymin / image_height) * self.height\n",
    "            yamx_final = (ymax / image_height) * self.height\n",
    "\n",
    "            boxes.append([xmin_final, ymin_final, xmax_final, yamx_final])\n",
    "\n",
    "        # bounding box to tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # area of the bounding boxes\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # no crowd instances\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "        # labels to tensor\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        # prepare the final `target` dictionary\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        image_id = torch.tensor([idx])\n",
    "        target[\"image_id\"] = image_id\n",
    "        # apply the image transforms\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(image=image_resized,\n",
    "                                     bboxes=target['boxes'],\n",
    "                                     labels=labels)\n",
    "            image_resized = sample['image']\n",
    "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
    "\n",
    "        return image_resized, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_images)\n",
    "\n",
    "\n",
    "\n",
    "# prepare the final datasets and data loaders\n",
    "train_dataset = CustomDataset(TRAIN_DIR, RESIZE_TO, RESIZE_TO, CLASSES, get_train_transform())\n",
    "valid_dataset = CustomDataset(VALID_DIR, RESIZE_TO, RESIZE_TO, CLASSES, get_valid_transform())\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(valid_dataset)}\\n\")\n",
    "\n",
    "# execute datasets.py using Python command from Terminal...\n",
    "# ... to visualize sample images\n",
    "# USAGE: python datasets.py\n",
    "if __name__ == '__main__':\n",
    "    # sanity check of the Dataset pipeline with sample visualization\n",
    "    dataset = CustomDataset(\n",
    "        TRAIN_DIR, RESIZE_TO, RESIZE_TO, CLASSES\n",
    "    )\n",
    "    print(f\"Number of training images: {len(dataset)}\")\n",
    "\n",
    "\n",
    "    # function to visualize a single sample\n",
    "    def visualize_sample(image, target):\n",
    "        box = target['boxes'][0]\n",
    "        label = CLASSES[target['labels'][0]]\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (int(box[0]), int(box[1])), (int(box[2]), int(box[3])),\n",
    "            (0, 255, 0), 1\n",
    "        )\n",
    "        cv2.putText(\n",
    "            image, label, (int(box[0]), int(box[1] - 5)),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2\n",
    "        )\n",
    "        cv2.imshow('Image', image)\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "\n",
    "    NUM_SAMPLES_TO_VISUALIZE = 200\n",
    "    for i in range(NUM_SAMPLES_TO_VISUALIZE):\n",
    "        image, target = dataset[i]\n",
    "        visualize_sample(image, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3d1ea32",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'image' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Now, you can use this function for visualization\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_SAMPLES_TO_VISUALIZE):\n\u001b[1;32m---> 28\u001b[0m     image, target \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     29\u001b[0m     visualize_sample(image, target)\n",
      "Cell \u001b[1;32mIn[6], line 39\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     36\u001b[0m min_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(image1)\n\u001b[0;32m     37\u001b[0m max_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(image1)\n\u001b[1;32m---> 39\u001b[0m scaled_image \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[0;32m     40\u001b[0m image \u001b[38;5;241m=\u001b[39m scaled_image\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     41\u001b[0m img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(image, dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'image' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_sample(image, target):\n",
    "    box = target['boxes'][0]\n",
    "    label = CLASSES[target['labels'][0]]\n",
    "\n",
    "    # Convert to np.uint8 before displaying\n",
    "    image_display = image.squeeze()\n",
    "\n",
    "    # Create a figure and axes\n",
    "    fig, ax = plt.subplots(1)\n",
    "    \n",
    "    # Display the image\n",
    "    ax.imshow(image_display)  # Add cmap='gray' if the image is grayscale\n",
    "\n",
    "    # Draw bounding box\n",
    "    rect = plt.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1],\n",
    "                         linewidth=1, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    # Add label\n",
    "    ax.text(box[0], box[1], label, bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Now, you can use this function for visualization\n",
    "for i in range(NUM_SAMPLES_TO_VISUALIZE):\n",
    "    image, target = dataset[i]\n",
    "    visualize_sample(image, target)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236a831f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14c829d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
